<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Academic</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Academic</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 02 Nov 2019 21:35:12 +0800</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Academic</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Automating Feature Engineering in Supervised Learning</title>
      <link>/post/automating-feature-engineering-in-supervised-learning/</link>
      <pubDate>Sat, 02 Nov 2019 21:35:12 +0800</pubDate>
      <guid>/post/automating-feature-engineering-in-supervised-learning/</guid>
      <description>&lt;h2 id=&#34;automating-feature-engineering-in-supervised-learning&#34;&gt;Automating Feature Engineering in Supervised Learning&lt;/h2&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;举了一个例子。通过用 $sin $ 进行映射，样本点可以被分组。&lt;/p&gt;
&lt;p&gt;$\to$ 定义一个映射函数&lt;/p&gt;
&lt;p&gt;举心脏病的例子,说明实际上真正重要的因素往往是几个主要因素的组合,如$ BMI $&lt;/p&gt;
&lt;p&gt;New tech consider the selection of features as black boxes, and is drien by the actual performance.&lt;/p&gt;
&lt;p&gt;$\to$ applying reinforcement learning&lt;/p&gt;
&lt;h3 id=&#34;challenges-in-performing-feature-engineering&#34;&gt;Challenges in Performing Feature Engineering&lt;/h3&gt;
&lt;p&gt;人工极大根据经验&lt;/p&gt;
&lt;p&gt;强化算法也不需要经验&lt;/p&gt;
&lt;h3 id=&#34;terminology-and-problem-definition&#34;&gt;Terminology and Problem Definition&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$F=\lbrace f_1,f_2,&amp;hellip;,f_m \rbrace$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A\ target\ vector\ y$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A\ suitable\ learning\ algorithm\ L$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A\ measurement\ of\ performance\ m$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$A_L^m(F,y)\ signify\ performance$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;for example, $L$ can be &lt;strong&gt;Logitic regression&lt;/strong&gt; and m is &lt;strong&gt;cross entropy&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$Assume\ a\ set\ of\ transformation\ function\ \lbrace t_1,t_2,&amp;hellip;t_m \rbrace $&lt;/p&gt;
&lt;p&gt;$$ f_{out}=t_i(f_in)\ where\ f_{in},f_{out} \in R^n$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;define set of operations: $+$&lt;/p&gt;
&lt;p&gt;aim:
$$
F^*=argmax_{F1,F2}A^m_L(F_1 \bigcup F_2)
\ F1\ from\ original\ dataset
\ F2\ form\ derived\ dataset
$$&lt;/p&gt;
&lt;h3 id=&#34;a-few-simple-approaches&#34;&gt;A Few Simple Approaches&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Apply all $Transformation\ Function$ to the given data and sum them up&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ad: Easy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dd:computation inefficiency &amp;amp; overfitting&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Every time add a new feature, train and evaluate&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;more scalable&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dd: also slow because the model training and evaluation&lt;/p&gt;
&lt;p&gt;refuce the deep composition of transforms&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/img/simple_way.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;hierarchical-exploration-of-feature-transformations&#34;&gt;Hierarchical Exploration of Feature Transformations&lt;/h3&gt;
&lt;p&gt;idea: batching of new features &amp;amp; hierarchical composition&lt;/p&gt;
&lt;h3 id=&#34;transformation-graph&#34;&gt;Transformation Graph&lt;/h3&gt;
&lt;p&gt;The transformation graph: $G$&lt;/p&gt;
&lt;p&gt;Given  Dataset:$D_0$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After each transformation, the target will not change&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The node type:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;start&lt;/li&gt;
&lt;li&gt;from $T(D_{before})$&lt;/li&gt;
&lt;li&gt;from $D_1+D_2$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\theta (G)$ refers to all nodes in $G$&lt;/p&gt;
&lt;p&gt;$\lambda(D_0,D_1)$ refers to the transformation function $T$ makes $D_0 \to D_1$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;the best solution is always among one of the nodes&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;/img/DAG.png&#34; alt=&#34;DAG&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;transformation-graph-exploration&#34;&gt;Transformation Graph Exploration&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;Dataset D_0, MAX_SEARCH_TIMES
# G_0 is start_gragh, D_0 is the start_dataset
# G_i refers to the graph after i-1 transformation
time =0
while time &amp;lt; MAX_SEARCH_TIMES:
    Nodes = θ(G_time)
    b_ratio = time / MAX_SEARCH_TIMES
    Nodes_plus,Transform_plus = argmax(n,t) Rank(G_time,Nodes,Transform,b_ratio)
    G_(time+1)=Apply Transform_plus on Nodes_plus
    time = time+1
 &#39;&#39;&#39;
 time can be any element through the exploration, like consuming time
 &#39;&#39;&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;learning-optimal-traversal-policy&#34;&gt;Learning Optimal Traversal Policy&lt;/h3&gt;
&lt;p&gt;consider the reinforcement as &lt;strong&gt;Markov Decision&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;the state at step i is divided into two parts:&lt;/p&gt;
&lt;p&gt;(a). $G_i$ after i node-addtions&lt;/p&gt;
&lt;p&gt;(b).the remaining budget $b_{ratio}=\frac {i}{B_{max}}$&lt;/p&gt;
&lt;p&gt;Let the entire set of states as $S$&lt;/p&gt;
&lt;p&gt;an action at step i is$&amp;lt;n,t&amp;gt;$&lt;/p&gt;
&lt;p&gt;$n$ is nodes to add while $t$ is transformation function in accord&lt;/p&gt;
&lt;p&gt;To each step i, we get a reward  $r_i$&lt;/p&gt;
&lt;p&gt;$$
r_i =max_{n&amp;rsquo;\in\theta(G_{i+1})}\ A(n&amp;rsquo;) - max_{n\in\theta(G_{i})}\ A(n)\ where\ r_0=0
$$
The cumulative reward over time from state $S_i$&lt;/p&gt;
&lt;p&gt;$$
R(S_i)=\sum_{j=1}^{B_{max}} \gamma ^i \cdot r_{i+j}
$$
We apply $Q-Learning$&lt;/p&gt;
&lt;p&gt;$$
Q(s,c)=r(s,c)+\gamma(R)^\prod (\delta(s,c))
$$
The aim is:&lt;/p&gt;
&lt;p&gt;$$
\prod^*(s,c)= arg\ max_cQ(S,C)
$$&lt;/p&gt;
&lt;p&gt;Give an approximate prediction:&lt;/p&gt;
&lt;p&gt;$$
Q(s,c)=w^c \cdot f(s) \where f(s)=f(g,n,t,d)
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Feature Engineering</title>
      <link>/post/learning-feature-engineering/</link>
      <pubDate>Wed, 23 Oct 2019 21:35:12 +0800</pubDate>
      <guid>/post/learning-feature-engineering/</guid>
      <description>&lt;h2 id=&#34;learning-feature-engineering-for-classification&#34;&gt;Learning Feature Engineering for Classification&lt;/h2&gt;
&lt;h3 id=&#34;the-structure&#34;&gt;The Structure&lt;/h3&gt;
&lt;p&gt;a stack of ﬁxed-size representations of feature values per target class&lt;/p&gt;
&lt;p&gt;passing the $LFE$ classifier requires a a ﬁxed size feature vector representation&lt;/p&gt;
&lt;h3 id=&#34;problem-formulation&#34;&gt;Problem Formulation&lt;/h3&gt;
&lt;p&gt;Consider a dataset $D$, with features, $F$=${f_1,&amp;hellip;,f_n }$&lt;/p&gt;
&lt;p&gt;A target class, ${ \kappa}$&lt;/p&gt;
&lt;p&gt;The transformation set  $T={T_1,&amp;hellip;T_m}$ and a classification task&lt;/p&gt;
&lt;p&gt;A classification task $L$&lt;/p&gt;
&lt;p&gt;The problem is to find $q$ best paradigm for constructing new features such that appending to the $D$ to maximizes the accuracy of $L$&lt;/p&gt;
&lt;p&gt;Each paradigm consists of a candidate transformation $T_c \in T$ of arity $r$ ,an ordered list of features $[ f_i,&amp;hellip;,f_{i+r-1}]$ and a usefulness score.&lt;/p&gt;
&lt;h3 id=&#34;transformation-recommendation&#34;&gt;Transformation Recommendation&lt;/h3&gt;
&lt;p&gt;LFE models the problem of predicting a useful r-ary transformation $T_c \in T_r$($ T_r\ is\ the\ set\ of\ transformation$) for a given list  $[f_1,&amp;hellip;,f_r]$&lt;/p&gt;
&lt;p&gt;The input is a  set of features&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;one vs rest approach:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each transformation is modelled as a MLP binary classification with real-valued confidence score as output.&lt;/p&gt;
&lt;p&gt;Apply the $|T_r|$ MLP on features, if the confidence score &amp;gt; threshold $\theta$ ,then choose this transformation.&lt;/p&gt;
&lt;p&gt;$$
c = arg\ max_k(R_{[f_1,&amp;hellip;,f_r]})\&lt;br&gt;
f(n)= \begin{cases} T_c, &amp;amp;\text {if $g_c$($R_{f_1,&amp;hellip;f_r}$) }&amp;gt;\theta
\ none,&amp;amp; \text{otherwise} \end{cases}
$$&lt;/p&gt;
&lt;h3 id=&#34;feature-class-representation&#34;&gt;Feature-Class Representation&lt;/h3&gt;
&lt;p&gt;The transformation are used to reveal and improve significance correlation or discriminative information between features and class labels.&lt;/p&gt;
&lt;p&gt;Correlation $\uparrow$ the effect $uparrow$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;work on improving the correlation!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;LFE represents feature $f$ in a dataset with $k$ classes as follows:&lt;/p&gt;
&lt;p&gt;$$
R_f=[Q_f^{(1)};Q_f^{(2)};&amp;hellip;;Q_f^{(k)};
$$&lt;/p&gt;
&lt;p&gt;$Q_f^{(i)}$ is a fixed-sized representation of values in $f$ that are associate with class$i$  $\to$ $\text {Quatile Sketch Array}$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The generation of $Q_f^{(i)}$&lt;/strong&gt; :**&lt;/p&gt;
&lt;p&gt;The high variability in size and range if feature values makes representative learning and PDF learning difficult.&lt;/p&gt;
&lt;p&gt;We consider features as high dimensions. We determine a fixed size to capture correlations between features and classes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Previous approaches:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Use meta-feature(the distribution of feature to transform the original features.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fixed-size sampling of both feature and classes. The samples need to reflect the distribution of features.&lt;/p&gt;
&lt;p&gt;Such stratified sampling is difficult to apply on the multiple features with high correlations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feature Hashing can be generated for numerical features but the appropriate function the map small range of feature values to the same hash features is difficult to find.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Quantile Sketching Array integrate these ways.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is familiar with histogram.&lt;/p&gt;
&lt;p&gt;Here we use brute-force way to find k-quantile-point.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Concrete production of  Quantile Sketch:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let $\nu_k$ be the bag of values of feature $f$ for training data point with label $c_k$ and $Q_f^{(i)}$ is the quantile sketch of $\nu_k$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Scale values to a predefined range $[lb,wb]$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bucket all values in $\nu_k$ into a set of bins.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For example, we partition $[lb,wb]$ into $r$ disjoint bins of uniform width,supposing width $w=\frac{wb-lb}r$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assume the bins are ${b_0,&amp;hellip;,b_{r-1}}$,$b_j$ ranges from $[lb+j*w,lb+(j+1)*w]$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;function $B(v_l)$ associates the value $v_l$ in $\nu_k$ to the bin $b_j$&lt;/p&gt;
&lt;p&gt;function $P(b_j)$ returns the number of feature values bucketed in $b_j$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally
$$
I(b_j)=\frac{P(b_j)}{\sum_{0\leq m&amp;lt;r}{P(b_m)}}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training&#34;&gt;Training&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Federated Forest</title>
      <link>/post/federated-forest/</link>
      <pubDate>Wed, 09 Oct 2019 01:13:34 +0800</pubDate>
      <guid>/post/federated-forest/</guid>
      <description>&lt;h2 id=&#34;federated-forest&#34;&gt;Federated Forest&lt;/h2&gt;
&lt;p&gt;joint model is necessary&lt;/p&gt;
&lt;p&gt;how?&lt;/p&gt;
&lt;p&gt;concept to model&lt;/p&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;四大贡献&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Secured privacy&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;每个节点对其他节点是透明的,信息交换尽可能小&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Loseless&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;适用于$vertical federated learning$,相比于集中式 $federated learning$ 可以做到无损&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;使用了MPI方法即时交换信息
简单不受限的预测算法(树的节点数,分裂数等)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;可行性和广泛性&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;支持分类和回归
易于实现,一样准确,有效率和鲁棒&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;related-work&#34;&gt;Related Work&lt;/h3&gt;
&lt;p&gt;其他论文的相关工作,引入模型,应用联邦学习,以及联邦学习的分类&lt;/p&gt;
&lt;h4 id=&#34;federated-learning&#34;&gt;Federated learning&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;meta learning&lt;/li&gt;
&lt;li&gt;multi-task&lt;/li&gt;
&lt;li&gt;AdaBoost&lt;/li&gt;
&lt;li&gt;logistic regression&lt;/li&gt;
&lt;li&gt;vertical, horizontal, transfer 三种分类&lt;/li&gt;
&lt;li&gt;secured federated transfer learning&lt;/li&gt;
&lt;li&gt;增强学习还有基于树的&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;data-privacy-protection&#34;&gt;Data Privacy Protection&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;differential privacy 差分隐私&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不增加计算负担&lt;/li&gt;
&lt;li&gt;降低模型表现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;homomorphic encryption 同态加密&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;算法复杂&lt;/li&gt;
&lt;li&gt;不支持none linear model(可以用泰勒展开近似)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;problem-formulation&#34;&gt;Problem Formulation&lt;/h3&gt;
&lt;p&gt;The model is assumed a vertical federated learning.&lt;/p&gt;
&lt;p&gt;There are M nodes&lt;/p&gt;
&lt;p&gt;To each node:&lt;/p&gt;
&lt;p&gt;$$
The\ data\ dormain\ is D_i
$$
To all models:&lt;/p&gt;
&lt;p&gt;$$
Tha\ data\ dormain\ is D=D_1 \small \bigcup D_2 \small \bigcup &amp;hellip;  \small \bigcup D_m
$$
where 1＜ i ＜ M&lt;/p&gt;
&lt;p&gt;$$
Denote\ the\ feature\ space\ of\ D_m\ as\ F_m:
$$&lt;/p&gt;
&lt;p&gt;All features&amp;rsquo; true name are encoded in order to protect privacy&lt;/p&gt;
&lt;p&gt;As assmuption, for 1＜ i,j＜ M:&lt;/p&gt;
&lt;p&gt;In the work, sample nums are same and IDs are in accord&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;现实中$M$通常很小&lt;/li&gt;
&lt;li&gt;不讨论$ID$如何对齐这种问题&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基本架构和 federated learning 基本一样,label y 由1个节点提供&lt;/p&gt;
&lt;h3 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Given:&lt;/strong&gt; Regional domain D~i~ and encrypted label $y$ on each client i, 1 ≤ i ≤ M.
&lt;strong&gt;Learn:&lt;/strong&gt; A Federated Forest, such that for each tree in the forest:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;a complete tree model T is held on master;&lt;/li&gt;
&lt;li&gt;a partial tree model Ti is stored on each client i, 1 ≤ i ≤ M&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Constraint:&lt;/strong&gt; The performance (accuracy, f1-score, MSE, e.t.c.) of the Federated Forest must be comparable to the non-federated random forest.&lt;/p&gt;
&lt;h3 id=&#34;methodology&#34;&gt;Methodology&lt;/h3&gt;
&lt;p&gt;对于单个节点有Alogorithm1&lt;/p&gt;
&lt;p&gt;Alogorithm 1:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;while 还有树要建立:
    if (node收到 Fi,Di):
    Function TreeBuild(){
        建立一个空的树节点
        if 满足剪枝条件 :
            设置为叶节点,voting得出label
        
        &#39;&#39;&#39;设置要记录的初始量&#39;&#39;&#39;
        purify,f_plus = -∞ , None
        &#39;&#39;&#39;
        如果master随机取到的fetures在该节点内还有没被作为分割点的
        注意这里是计算,并不划分
        &#39;&#39;&#39;
        if Fi != None:
            计算每个节点做分割点的信息增益,加入impurity_improvements
            &#39;&#39;&#39;得到最好的,f_plus记录该最好的节点&#39;&#39;&#39;
            best_purity = MAX(impurity_improvements)
            f_plus=getNodeOf(best_purity)
        
        把 best_purity发给master

        &#39;&#39;&#39;master告诉该节点这是全局最优&#39;&#39;&#39;
        if 从master接收到的split_message:
            &#39;&#39;&#39;节点正式划分&#39;&#39;&#39;
            is_selected=True
            划分样本
            把 left_tree, right_tree发给master
        else:
        &#39;&#39;&#39;说明最优划分节点不是这课树&#39;&#39;&#39;
            收到left_tree,right_tree各有哪些节点
        
        &#39;&#39;&#39;递归向下建树&#39;&#39;&#39;
        left_tree=TreeBuild(left_tree)
        right_tree=TreeBuild(right_tree)
        return node
    &#39;&#39;&#39;
    一棵树建立完了,加入森林
    可以把这个算法理解为,将计算信息增益的任务平摊到了每个节点
    &#39;&#39;&#39;
    Forests.append(Tree)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;针对master节点
Alogorithm 2:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;while 还需要建树 :
    随机取D,F
    对每个节点,把该节点的Fi发给该节点
    Fuction TreeBuild(D,F,y):
        if 满足剪枝条件:
            设置为叶节点,voting得出label
        
        接受来自各个节点的增益值,node_best_purities
        global_best_purity=MAX(node_best_purities)
        &#39;&#39;&#39;确定最优划分特征的来源节点&#39;&#39;&#39;
        selected_node=getNodeOf(global_best_purity)

        for node in nodes:
            if node is selected_node:
                发split_messagte表示它被选中
            else:
                把划分结果left_tree,right_tree告知node
        
    返回node根节点
    Forest.append(Tree)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;单个client无法包含一个划分节点的特征的全部信息,但每个client的森林结构是一样的.&lt;/p&gt;
&lt;p&gt;针对client节点的预测
Alogorithm 3:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;while 需要预测:
    TreePrediction(Ti,Di_test,Fi):
        &#39;&#39;&#39;Si是要预测的一个样本,in leaf表示预测完成&#39;&#39;&#39;
        if Si is in leaf:
            return (Si,Labeli)
        else:
            if 节点保存了划分信息(最优节点) :
                划分
                left_tree=TreePrediction(Ti_left,Di_test_left,Fi)
                right_tree=TreePrediction(Ti_right,Di_test_right,Fi)
            else:
                left_tree=TreePrediction(Ti_left,Di_test_left,Fi)
                right_tree=TreePrediction(Ti_right,Di_test_right,Fi)
            return (left_tree,right_tree)
        
        把 (S1,S2,...,Sm) 送到master
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;根据论文,如果一个样本遇到有划分标准的节点,则划分,否则同时全部进入左右两个节点&lt;/p&gt;
&lt;p&gt;对于决策树 $Ti$ 的每个叶节点,都会有一批样本,对第l个叶节点里的样本,记作 $S_i^l$, $(l \in L$, $L$是$T_i$的叶节点集合)&lt;/p&gt;
&lt;p&gt;对&lt;/p&gt;
&lt;p&gt;$$
{S_i^l}_{i=1}^M
$$
取交集运算&lt;/p&gt;
&lt;p&gt;每个节点的森林的结构是一样的,因此所有叶节点的位置和数目一一对应,可以做交集运算&lt;/p&gt;
&lt;p&gt;虽然论文 appendix 的证明正规冗长,理解起来是不难的:
可以理解为:&lt;/p&gt;
&lt;p&gt;$$
S\small\bigcap A\small\bigcap B\small\bigcap C = (S\small\bigcap A\small\bigcap B)\small\bigcap (S\small\bigcap C)
$$
$$\text{S是样本全集. A,B,C是分别满足条件a,b,c的样本集合}$$&lt;/p&gt;
&lt;p&gt;Algorithm 4:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;while 需要预测:
    Gather{S1,S2,...Sm}
    Obtain(S^1,S^2,...S^m)
Return all the label 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;privacy-protection&#34;&gt;Privacy Protection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Identities:&lt;/strong&gt; 对用户进行hash编码然后进行MD5加密&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Label:&lt;/strong&gt; encode和同态加密的权衡
&lt;strong&gt;Feature:&lt;/strong&gt; feature名进行加密
&lt;strong&gt;Communication:&lt;/strong&gt; RSA或ADS
&lt;strong&gt;Moddel Storage:&lt;/strong&gt;  Federated Learning​ 的细节已经实现对模型的保护&lt;/p&gt;
&lt;h3 id=&#34;experiment&#34;&gt;Experiment&lt;/h3&gt;
&lt;p&gt;重点不是表现多好&lt;/p&gt;
&lt;p&gt;而是Federated Learning​ 和 Classical Random Forest 表现的比较&lt;/p&gt;
&lt;h3 id=&#34;heading&#34;&gt;结论&lt;/h3&gt;
&lt;p&gt;Federated Forest 相比于 Classical Random Forest 速度快且无损.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Federated Learning:Challenges,Methods and Future Directions</title>
      <link>/post/federated-learning-challenges-methods-and-future-directions/</link>
      <pubDate>Sat, 28 Sep 2019 18:06:09 +0800</pubDate>
      <guid>/post/federated-learning-challenges-methods-and-future-directions/</guid>
      <description>&lt;h2 id=&#34;federated-learningchallengesmethods-and-future-directions&#34;&gt;Federated Learning:Challenges,Methods and Future Directions&lt;/h2&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;说明联盟学习的主要挑战,以及提供几个研究方向.&lt;/p&gt;
&lt;h3 id=&#34;introductions&#34;&gt;Introductions&lt;/h3&gt;
&lt;p&gt;设备计算力的提升 &amp;mdash;-&amp;gt; 边缘计算 &amp;mdash;-&amp;gt; 联盟学习
描述了一下联盟学习的流程
☆ 特别用于对于隐私数据的训练,情感分析,用户踪迹预测等&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点范围: 个人,组织,物联网&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;problem-formulation&#34;&gt;Problem Formulation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;前提:数据不出本地&lt;/li&gt;
&lt;li&gt;定义损失函数&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$$ \min_w F(w), \ where \ F(w):=\sum_{k=1}^mp_kF_k(w)$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$\sum_{k=1}^mp_k=1$, 相当于各个设备损失函数的加权平均数&lt;/p&gt;
&lt;h4 id=&#34;core-challenges&#34;&gt;Core Challenges&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Challenge 1: Expensive Communication&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少轮次&lt;/li&gt;
&lt;li&gt;减少传输数据量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Challenge 2: Systems Heterogeneity&lt;/strong&gt;
设备条件不同,数量众多,在训练中容易出现故障
需要做到:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;容错&lt;/li&gt;
&lt;li&gt;允许丢失设备,并支持后续处理&lt;/li&gt;
&lt;li&gt;对参与计算的设备少能够有所准备&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Challenge 3: Statistical Heterogeneity&lt;/strong&gt;
数据不再满足局部性原理&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Challenge 4: Privacy Concerns&lt;/strong&gt;
隐私问题&lt;/p&gt;
&lt;h4 id=&#34;related-and-current-work&#34;&gt;Related and Current Work&lt;/h4&gt;
&lt;h5 id=&#34;communication-efficiency&#34;&gt;Communication Efficiency&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;把一个问题分成若干个子问题
交由不同的设备处理&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ADMM问题?&lt;/p&gt;
&lt;p&gt;Federated Averaging适合非凸优化问题,即存在全局最优解的问题&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/pysyft-notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/pysyft-notes/</guid>
      <description>&lt;p&gt;Hook shows the usage of template method, a design pattern.&lt;/p&gt;
&lt;p&gt;It extend the function and interfaces of torch.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;bob = sy.VirtualWorker(hook, id=&amp;quot;bob&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define a node participating in federated training.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
