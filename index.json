[{"authors":["admin"],"categories":null,"content":" In the dark night, longing for a light of care\nIn the daylight, desiring for the swallow of a black hole\nCause curiosity — seeks for what's in the darkness\n Hi. My name is Pei Fang (方沛). I am a PhD student at the Electric and Computer Engineering, where I worked on Federated Learning and AutoML. My advisor is Mi Zhang.\nI got my B.E. in Software Engineering from Tongji University. Besides, I collaborated with Pengyuan Zhou on Cross-device Federated Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"In the dark night, longing for a light of care\nIn the daylight, desiring for the swallow of a black hole\nCause curiosity — seeks for what's in the darkness\n Hi. My name is Pei Fang (方沛). I am a PhD student at the Electric and Computer Engineering, where I worked on Federated Learning and AutoML. My advisor is Mi Zhang.\nI got my B.E. in Software Engineering from Tongji University.","tags":null,"title":"Pei Fang","type":"authors"},{"authors":null,"categories":["Federated Learning","PySyft"],"content":"hook = sy.TorchHook(torch)\r Hook shows the usage of template method, a design pattern.\nIt extend the function and interfaces of torch.\nbob = sy.VirtualWorker(hook, id=\u0026quot;bob\u0026quot;)\r Define a node participating in federated training.\nx = torch.tensor([1,2,3,4,5])\ry = torch.tensor([1,1,1,1,1])\r# x y都是本地的数据\rz = x + y # z 也是本地的\r# 将x发送到alice、y发送到bob\rx_ptr = x.send(alice)\ry_ptr = y.send(bob)\r# 这一句不能执行，因为x_ptr是alice的数据，y_ptr是bob的数据\rz = x_ptr + y\r# 可以执行，x_ptr和y_ptr此时都在bob上\rx_ptr = x.send(bob)\rz = x_ptr+y_ptr\r Only the manipulation between tensors in the same agent can be executed.\n","date":1579627519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579627519,"objectID":"88cb48c88e8b97069b64ab06c1cb0c23","permalink":"/post/pysyft-notes/","publishdate":"2020-01-22T01:25:19+08:00","relpermalink":"/post/pysyft-notes/","section":"post","summary":"To be continue...","tags":null,"title":"PySyft Notes","type":"post"},{"authors":null,"categories":["Reinforcement Learning","Feature Engineering","reproduction"],"content":"Automating Feature Engineering in Supervised Learning Introduction 举了一个例子。通过用 $sin $ 进行映射，样本点可以被分组。\n$\\to$ 定义一个映射函数\n举心脏病的例子,说明实际上真正重要的因素往往是几个主要因素的组合,如$ BMI $\nNew tech consider the selection of features as black boxes, and is drien by the actual performance.\n$\\to$ applying reinforcement learning\nChallenges in Performing Feature Engineering 人工极大根据经验\n强化算法也不需要经验\nTerminology and Problem Definition   $F=\\lbrace f_1,f_2,\u0026hellip;,f_m \\rbrace$\n  $A\\ target\\ vector\\ y$\n  $A\\ suitable\\ learning\\ algorithm\\ L$\n  $A\\ measurement\\ of\\ performance\\ m$\n  $A_L^m(F,y)\\ signify\\ performance$\n for example, $L$ can be Logitic regression and m is cross entropy\n   $Assume\\ a\\ set\\ of\\ transformation\\ function\\ \\lbrace t_1,t_2,\u0026hellip;t_m \\rbrace $\n$$ f_{out}=t_i(f_in)\\ where\\ f_{in},f_{out} \\in R^n$$\n  define set of operations: $+$\naim: $$ F^*=argmax_{F1,F2}A^m_L(F_1 \\bigcup F_2) \\ F1\\ from\\ original\\ dataset \\ F2\\ form\\ derived\\ dataset $$\nA Few Simple Approaches   Apply all $Transformation\\ Function$ to the given data and sum them up\n  Ad: Easy\n  Dd:computation inefficiency \u0026amp; overfitting\n    Every time add a new feature, train and evaluate\n  more scalable\n  Dd: also slow because the model training and evaluation\nrefuce the deep composition of transforms\n    Hierarchical Exploration of Feature Transformations idea: batching of new features \u0026amp; hierarchical composition\nTransformation Graph The transformation graph: $G$\nGiven Dataset:$D_0$\n After each transformation, the target will not change\n The node type:\n start from $T(D_{before})$ from $D_1+D_2$  $\\theta (G)$ refers to all nodes in $G$\n$\\lambda(D_0,D_1)$ refers to the transformation function $T$ makes $D_0 \\to D_1$\n the best solution is always among one of the nodes\n Transformation Graph Exploration Dataset D_0, MAX_SEARCH_TIMES # G_0 is start_gragh, D_0 is the start_dataset # G_i refers to the graph after i-1 transformation time =0 while time \u0026lt; MAX_SEARCH_TIMES: Nodes = θ(G_time) b_ratio = time / MAX_SEARCH_TIMES Nodes_plus,Transform_plus = argmax(n,t) Rank(G_time,Nodes,Transform,b_ratio) G_(time+1)=Apply Transform_plus on Nodes_plus time = time+1 ''' time can be any element through the exploration, like consuming time '''  Learning Optimal Traversal Policy consider the reinforcement as Markov Decision\nthe state at step i is divided into two parts:\n(a). $G_i$ after i node-addtions\n(b).the remaining budget $b_{ratio}=\\frac {i}{B_{max}}$\nLet the entire set of states as $S$\nan action at step i is$\u0026lt;n,t\u0026gt;$\n$n$ is nodes to add while $t$ is transformation function in accord\nTo each step i, we get a reward $r_i$\n$$ r_i =max_{n\u0026rsquo;\\in\\theta(G_{i+1})}\\ A(n\u0026rsquo;) - max_{n\\in\\theta(G_{i})}\\ A(n)\\ where\\ r_0=0 $$ The cumulative reward over time from state $S_i$\n$$ R(S_i)=\\sum_{j=1}^{B_{max}} \\gamma ^i \\cdot r_{i+j} $$ We apply $Q-Learning$\n$$ Q(s,c)=r(s,c)+\\gamma(R)^\\prod (\\delta(s,c)) $$ The aim is:\n$$ \\prod^*(s,c)= arg\\ max_cQ(S,C) $$\nGive an approximate prediction:\n$$ Q(s,c)=w^c \\cdot f(s) \\where f(s)=f(g,n,t,d) $$\n","date":1572701712,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572701712,"objectID":"07b5f58541c4d7e7dcc619b63a20786a","permalink":"/post/automating-feature-engineering-in-supervised-learning/","publishdate":"2019-11-02T21:35:12+08:00","relpermalink":"/post/automating-feature-engineering-in-supervised-learning/","section":"post","summary":"List some practical methods for automated feature engineering, DAG, Q-learning....","tags":null,"title":"Automating Feature Engineering in Supervised Learning","type":"post"},{"authors":null,"categories":["Deep Learning","Feature Engineering"],"content":"Learning Feature Engineering for Classification The Structure a stack of ﬁxed-size representations of feature values per target class\npassing the $LFE$ classifier requires a a ﬁxed size feature vector representation\nProblem Formulation Consider a dataset $D$, with features, $F$=${f_1,\u0026hellip;,f_n }$\nA target class, ${ \\kappa}$\nThe transformation set $T={T_1,\u0026hellip;T_m}$ and a classification task\nA classification task $L$\nThe problem is to find $q$ best paradigm for constructing new features such that appending to the $D$ to maximizes the accuracy of $L$\nEach paradigm consists of a candidate transformation $T_c \\in T$ of arity $r$ ,an ordered list of features $[ f_i,\u0026hellip;,f_{i+r-1}]$ and a usefulness score.\nTransformation Recommendation LFE models the problem of predicting a useful r-ary transformation $T_c \\in T_r$($ T_r\\ is\\ the\\ set\\ of\\ transformation$) for a given list $[f_1,\u0026hellip;,f_r]$\nThe input is a set of features\none vs rest approach:\nEach transformation is modelled as a MLP binary classification with real-valued confidence score as output.\nApply the $|T_r|$ MLP on features, if the confidence score \u0026gt; threshold $\\theta$ ,then choose this transformation.\n$$ c = arg\\ max_k(R_{[f_1,\u0026hellip;,f_r]})\\\nf(n)= \\begin{cases} T_c, \u0026amp;\\text {if $g_c$($R_{f_1,\u0026hellip;f_r}$) }\u0026gt;\\theta \\ none,\u0026amp; \\text{otherwise} \\end{cases} $$\nFeature-Class Representation The transformation are used to reveal and improve significance correlation or discriminative information between features and class labels.\nCorrelation $\\uparrow$ the effect $uparrow$\n work on improving the correlation!\n LFE represents feature $f$ in a dataset with $k$ classes as follows:\n$$ R_f=[Q_f^{(1)};Q_f^{(2)};\u0026hellip;;Q_f^{(k)}; $$\n$Q_f^{(i)}$ is a fixed-sized representation of values in $f$ that are associate with class$i$ $\\to$ $\\text {Quatile Sketch Array}$\nThe generation of $Q_f^{(i)}$ :**\nThe high variability in size and range if feature values makes representative learning and PDF learning difficult.\nWe consider features as high dimensions. We determine a fixed size to capture correlations between features and classes.\nPrevious approaches:\n  Use meta-feature(the distribution of feature to transform the original features.)\n  Fixed-size sampling of both feature and classes. The samples need to reflect the distribution of features.\nSuch stratified sampling is difficult to apply on the multiple features with high correlations.\n  Feature Hashing can be generated for numerical features but the appropriate function the map small range of feature values to the same hash features is difficult to find.\n   Quantile Sketching Array integrate these ways.\nIt is familiar with histogram.\nHere we use brute-force way to find k-quantile-point.\n Concrete production of Quantile Sketch:\nLet $\\nu_k$ be the bag of values of feature $f$ for training data point with label $c_k$ and $Q_f^{(i)}$ is the quantile sketch of $\\nu_k$\n  Scale values to a predefined range $[lb,wb]$\n  Bucket all values in $\\nu_k$ into a set of bins.\n  For example, we partition $[lb,wb]$ into $r$ disjoint bins of uniform width,supposing width $w=\\frac{wb-lb}r$\n  Assume the bins are ${b_0,\u0026hellip;,b_{r-1}}$,$b_j$ ranges from $[lb+j*w,lb+(j+1)*w]$.\n  function $B(v_l)$ associates the value $v_l$ in $\\nu_k$ to the bin $b_j$\nfunction $P(b_j)$ returns the number of feature values bucketed in $b_j$\n  Finally $$ I(b_j)=\\frac{P(b_j)}{\\sum_{0\\leq m\u0026lt;r}{P(b_m)}} $$\n  Training ","date":1571837712,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571837712,"objectID":"914135f4491e80c5f82ef8162677e963","permalink":"/post/learning-feature-engineering/","publishdate":"2019-10-23T21:35:12+08:00","relpermalink":"/post/learning-feature-engineering/","section":"post","summary":"A new feature engineering idea without model evaluation.","tags":null,"title":"Learning Feature Engineering","type":"post"},{"authors":null,"categories":["Machine Learning","Federated Learning","reproduction"],"content":"Federated Forest joint model is necessary\nhow?\nconcept to model\nIntroduction 四大贡献\n Secured privacy   每个节点对其他节点是透明的,信息交换尽可能小\n  Loseless   适用于$vertical federated learning$,相比于集中式 $federated learning$ 可以做到无损\n  Efficiency   使用了MPI方法即时交换信息 简单不受限的预测算法(树的节点数,分裂数等)\n  可行性和广泛性   支持分类和回归 易于实现,一样准确,有效率和鲁棒\n Related Work 其他论文的相关工作,引入模型,应用联邦学习,以及联邦学习的分类\nFederated learning  meta learning multi-task AdaBoost logistic regression vertical, horizontal, transfer 三种分类 secured federated transfer learning 增强学习还有基于树的  Data Privacy Protection differential privacy 差分隐私\n 不增加计算负担 降低模型表现  homomorphic encryption 同态加密\n 算法复杂 不支持none linear model(可以用泰勒展开近似)  Problem Formulation The model is assumed a vertical federated learning.\nThere are M nodes\nTo each node:\n$$ The\\ data\\ dormain\\ is D_i $$ To all models:\n$$ Tha\\ data\\ dormain\\ is D=D_1 \\small \\bigcup D_2 \\small \\bigcup \u0026hellip; \\small \\bigcup D_m $$ where 1＜ i ＜ M\n$$ Denote\\ the\\ feature\\ space\\ of\\ D_m\\ as\\ F_m: $$\nAll features\u0026rsquo; true name are encoded in order to protect privacy\nAs assmuption, for 1＜ i,j＜ M:\nIn the work, sample nums are same and IDs are in accord\nNotation\n 现实中$M$通常很小 不讨论$ID$如何对齐这种问题  基本架构和 federated learning 基本一样,label y 由1个节点提供\nProblem Statement Given: Regional domain D~i~ and encrypted label $y$ on each client i, 1 ≤ i ≤ M. Learn: A Federated Forest, such that for each tree in the forest:\n a complete tree model T is held on master; a partial tree model Ti is stored on each client i, 1 ≤ i ≤ M  Constraint: The performance (accuracy, f1-score, MSE, e.t.c.) of the Federated Forest must be comparable to the non-federated random forest.\nMethodology 对于单个节点有Alogorithm1\nAlogorithm 1:\nwhile 还有树要建立: if (node收到 Fi,Di): Function TreeBuild(){ 建立一个空的树节点 if 满足剪枝条件 : 设置为叶节点,voting得出label '''设置要记录的初始量''' purify,f_plus = -∞ , None ''' 如果master随机取到的fetures在该节点内还有没被作为分割点的 注意这里是计算,并不划分 ''' if Fi != None: 计算每个节点做分割点的信息增益,加入impurity_improvements '''得到最好的,f_plus记录该最好的节点''' best_purity = MAX(impurity_improvements) f_plus=getNodeOf(best_purity) 把 best_purity发给master '''master告诉该节点这是全局最优''' if 从master接收到的split_message: '''节点正式划分''' is_selected=True 划分样本 把 left_tree, right_tree发给master else: '''说明最优划分节点不是这课树''' 收到left_tree,right_tree各有哪些节点 '''递归向下建树''' left_tree=TreeBuild(left_tree) right_tree=TreeBuild(right_tree) return node ''' 一棵树建立完了,加入森林 可以把这个算法理解为,将计算信息增益的任务平摊到了每个节点 ''' Forests.append(Tree) }  针对master节点 Alogorithm 2:\nwhile 还需要建树 : 随机取D,F 对每个节点,把该节点的Fi发给该节点 Fuction TreeBuild(D,F,y): if 满足剪枝条件: 设置为叶节点,voting得出label 接受来自各个节点的增益值,node_best_purities global_best_purity=MAX(node_best_purities) '''确定最优划分特征的来源节点''' selected_node=getNodeOf(global_best_purity) for node in nodes: if node is selected_node: 发split_messagte表示它被选中 else: 把划分结果left_tree,right_tree告知node 返回node根节点 Forest.append(Tree)  单个client无法包含一个划分节点的特征的全部信息,但每个client的森林结构是一样的.\n针对client节点的预测 Alogorithm 3:\nwhile 需要预测: TreePrediction(Ti,Di_test,Fi): '''Si是要预测的一个样本,in leaf表示预测完成''' if Si is in leaf: return (Si,Labeli) else: if 节点保存了划分信息(最优节点) : 划分 left_tree=TreePrediction(Ti_left,Di_test_left,Fi) right_tree=TreePrediction(Ti_right,Di_test_right,Fi) else: left_tree=TreePrediction(Ti_left,Di_test_left,Fi) right_tree=TreePrediction(Ti_right,Di_test_right,Fi) return (left_tree,right_tree) 把 (S1,S2,...,Sm) 送到master  根据论文,如果一个样本遇到有划分标准的节点,则划分,否则同时全部进入左右两个节点\n对于决策树 $Ti$ 的每个叶节点,都会有一批样本,对第l个叶节点里的样本,记作 $S_i^l$, $(l \\in L$, $L$是$T_i$的叶节点集合)\n对\n$$ {S_i^l}_{i=1}^M $$ 取交集运算\n每个节点的森林的结构是一样的,因此所有叶节点的位置和数目一一对应,可以做交集运算\n虽然论文 appendix 的证明正规冗长,理解起来是不难的: 可以理解为:\n$$ S\\small\\bigcap A\\small\\bigcap B\\small\\bigcap C = (S\\small\\bigcap A\\small\\bigcap B)\\small\\bigcap (S\\small\\bigcap C) $$ $$\\text{S是样本全集. A,B,C是分别满足条件a,b,c的样本集合}$$\nAlgorithm 4:\nwhile 需要预测: Gather{S1,S2,...Sm} Obtain(S^1,S^2,...S^m) Return all the label  Privacy Protection Identities: 对用户进行hash编码然后进行MD5加密\nLabel: encode和同态加密的权衡 Feature: feature名进行加密 Communication: RSA或ADS Moddel Storage: Federated Learning​ 的细节已经实现对模型的保护\nExperiment 重点不是表现多好\n而是Federated Learning​ 和 Classical Random Forest 表现的比较\n结论 Federated Forest 相比于 Classical Random Forest 速度快且无损.\n","date":1570554814,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570554814,"objectID":"16840c3725c4dd54b0083e7b5d6c1a76","permalink":"/post/federated-forest/","publishdate":"2019-10-09T01:13:34+08:00","relpermalink":"/post/federated-forest/","section":"post","summary":"A federated version CART. Generally it is loseless and faster.","tags":null,"title":"Federated Forest","type":"post"},{"authors":null,"categories":["Federated Learning"],"content":"Federated Learning:Challenges,Methods and Future Directions Abstract 说明联盟学习的主要挑战,以及提供几个研究方向.\nIntroductions 设备计算力的提升 \u0026mdash;-\u0026gt; 边缘计算 \u0026mdash;-\u0026gt; 联盟学习 描述了一下联盟学习的流程 ☆ 特别用于对于隐私数据的训练,情感分析,用户踪迹预测等\n 节点范围: 个人,组织,物联网  Problem Formulation  前提:数据不出本地 定义损失函数   $$ \\min_w F(w), \\ where \\ F(w):=\\sum_{k=1}^mp_kF_k(w)$$\n $\\sum_{k=1}^mp_k=1$, 相当于各个设备损失函数的加权平均数\nCore Challenges Challenge 1: Expensive Communication\n 减少轮次 减少传输数据量  Challenge 2: Systems Heterogeneity 设备条件不同,数量众多,在训练中容易出现故障 需要做到:\n 容错 允许丢失设备,并支持后续处理 对参与计算的设备少能够有所准备  Challenge 3: Statistical Heterogeneity 数据不再满足局部性原理\nChallenge 4: Privacy Concerns 隐私问题\nRelated and Current Work Communication Efficiency  把一个问题分成若干个子问题 交由不同的设备处理\n ADMM问题?\nFederated Averaging适合非凸优化问题,即存在全局最优解的问题\n","date":1569665169,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569665169,"objectID":"6ad9ffede79998cc6cda0a4810ae9e6f","permalink":"/post/federated-learning-challenges-methods-and-future-directions/","publishdate":"2019-09-28T18:06:09+08:00","relpermalink":"/post/federated-learning-challenges-methods-and-future-directions/","section":"post","summary":"As title","tags":null,"title":"Federated Learning:Challenges,Methods and Future Directions","type":"post"}]